{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f973c9-87f6-49f6-973d-b86ff0abd4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from IPython.display import Markdown,display,update_display\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92addd3-cd6a-4bd9-a4d5-2e606ecd8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override = True)\n",
    "URL = os.getenv('URL')\n",
    "URL\n",
    "MODEL = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a18d892-2f38-49d4-92e1-4d5fba06a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(base_url = 'http://localhost:11434/v1',api_key = 'ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0483557a-4389-4a99-8d4d-b6cbe61af56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an helpful Technical tutor for engineering students\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4e6f9a3-d72b-4eac-acc8-224caf53df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_user():\n",
    "    return input(\"\\n You:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2558589-b21e-4129-8e20-88edb8c60f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_a():\n",
    "    user_prompt = ask_user()\n",
    "    response = openai.chat.completions.create(\n",
    "        model = MODEL,\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":system_prompt},\n",
    "            {\"role\":\"user\",\"content\":user_prompt}\n",
    "        ],\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    display(Markdown(result))\n",
    "    #print(f\"Tutor:{result}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c2dc623-457f-48e2-804a-095a1fa3704c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " You: Explain Backpropogation?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Backpropagation (BP) Algorithm**\n",
       "\n",
       "Backpropagation is a widely used algorithm in machine learning and neural networks to train artificial neural networks. It's a crucial algorithm that enables the network to learn from its errors and adjust its parameters to minimize the difference between the predicted output and the actual output.\n",
       "\n",
       "**What is Backpropagation?**\n",
       "\n",
       "In simple terms, backpropagation is a method of training a neural network by propagating the error gradient backwards through the network. This gradient is a measure of how sensitive each parameter (weight) is to changes in the loss function.\n",
       "\n",
       "The process involves the following steps:\n",
       "\n",
       "1. **Forward Pass**: First, the inputs are fed into the network, and the output is computed.\n",
       "2. **Error Calculation**: The difference between the predicted output and the actual output is calculated (this is called the error).\n",
       "3. **Error Gradient**: The error gradient (dE/dw), which represents the rate of change of the loss function with respect to each parameter (weight) is computed using backpropagation formulas.\n",
       "4. **Weight Update**: Each parameter (weight) is updated based on the error gradient and a learning rate.\n",
       "\n",
       "**Backpropagation Formulas**\n",
       "\n",
       "The backpropagation process uses the following formulas:\n",
       "\n",
       "1. `dE/dx = dL/dy * dy/dx`\n",
       "2. `dp/dw = dE/db + dw * dB/dw`\n",
       "\n",
       "These formulas compute the gradient of the loss function with respect to each parameter.\n",
       "\n",
       "**How Backpropagation Works**\n",
       "\n",
       "Here's a step-by-step example of how backpropagation works:\n",
       "\n",
       "1. Suppose we have a neural network with two input nodes, one hidden node, and one output node.\n",
       "2. The inputs are fed into the network, and the output is computed using a sigmoid activation function (`f(x) = 1/(1+e^(-x))`).\n",
       "3. The actual output is compared to the predicted output, and a small value of `ε` (small value) is added to the predicted output.\n",
       "4. The error gradient (dL/dw) is computed using backpropagation formulas.\n",
       "5. For each parameter (weight), we multiply it with the corresponding error gradient (`dp/dw`) and add the result with itself (`dL/db + w * dB/dw`).\n",
       "6. The resulting weights are then updated based on the learning rate.\n",
       "\n",
       "**Example**\n",
       "\n",
       "Consider a neural network with one input node, one hidden node, and two output nodes. We want to train the following network:\n",
       "\n",
       "`Y = f(φ(x))`\n",
       "\n",
       "Where:\n",
       "- `x` is the input\n",
       "- `φ(x)` represents the output of the sigmoid activation function (`1/(1+e^(-x))`)\n",
       "- `y` represents one of the two possible outputs (0 or 1)\n",
       "- `W_xi` and `W_xy` are weights that need to be learned\n",
       "\n",
       "**Error Computation**\n",
       "\n",
       "Let's compute the error for one of the two output nodes. We have:\n",
       "\n",
       "```python\n",
       "ε = 0.5\n",
       "y_pred = sigmoid(φ(x))\n",
       "error = (1-y_true)*dL/dy_pred + ε * (0.01 - y_true)\n",
       "```\n",
       "\n",
       "We need to propagate this error backwards through the network.\n",
       "\n",
       "**Backpropagation**\n",
       "\n",
       "The partial derivatives are computed:\n",
       "\n",
       "```python\n",
       "dx_1_dE = dL/df(dL/db) * db/df(x)\n",
       "dW_xy_db = epsilon*(1-y_true)\n",
       "\n",
       "dW_xi_dx = dL/db *.db.dx\n",
       "```\n",
       "\n",
       "We can then update the weights using the following formulas:\n",
       "\n",
       "`dW_xy_db = 0.01*dL/dy`\n",
       "`dW_xi_dx = - (1- sigmoid(x))*(sigmoid(x)-y)`\n",
       "\n",
       "The weights are updated based on these partial derivatives.\n",
       "\n",
       "**Conclusion**\n",
       "\n",
       "Backpropagation is a fundamental algorithm used in machine learning and neural networks to train artificial neural networks. It optimizes the weights of the network by propagating the error gradient backwards through the network. This process enables the network to learn from its errors and make accurate predictions.\n",
       "\n",
       "Let me know if you want any code examples for backpropagation.\n",
       "\n",
       "**Code Example**\n",
       "\n",
       "Here's a Python example that demonstrates how backpropagation works using NumPy:\n",
       "\n",
       "```python\n",
       "import numpy as np\n",
       "\n",
       "# Define the activation function\n",
       "def sigmoid(x):\n",
       "    return 1 / (1 + np.exp(-x))\n",
       "\n",
       "# Initialize weights\n",
       "np.random.seed(0)\n",
       "X = np.array([[0.5, 0.2], [0.1, 0.7]])\n",
       "w1 = np.random.rand(3, 3)  \n",
       "b1 = np.zeros((1, 3))\n",
       "\n",
       "# Forward pass\n",
       "Z_w1x = np.dot(X, w1 )+ b1\n",
       "\n",
       "# Define the loss function\n",
       "def loss(z):\n",
       "    L = -np.mean([z[0][i] * (y_train[i] - sigma(z[0][i])) - y_train[i]*sigma(z[0][i])], axis=0)\n",
       "    return np.mean(L)\n",
       "\n",
       "# Derivative of activation function \n",
       "delta_W_x = 1 / ((Z_w1x.T)*X-1)\n",
       "\n",
       "dy_drn_b1 = np.ones(Z_w1x.shape)[0]\n",
       "dr_n_dz = -np.dot(X, w1)\n",
       "db_dx1 = sigma(W_0x, b1)\n",
       "\n",
       "def delta_b1(drn):\n",
       "    return  X.T @ drn\n",
       "\n",
       "def W_01deltawx(drw):\n",
       "\n",
       "    return np.mean([Z_w1x.T*deltawx])\n",
       "\n",
       "b0_dr1= Z_00-((y_train)\n",
       "delteWb1 = sigma(Z_bbx)\n",
       "\n",
       "# Back Prop\n",
       "drn = - (sigma(Z_bbx))-(y_train\n",
       "\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741198ca-2597-4684-b758-d423d1d2e5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
